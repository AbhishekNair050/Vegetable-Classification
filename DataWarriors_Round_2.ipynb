{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "5bbW22rYXa3_",
        "MtLQ1eHiat6_",
        "xf8wY0xtKNB7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Datathon 2.0 - Round 2\n",
        "## DataWarriors -\n",
        "## - Abhishek Nair\n",
        "## - priyanshi Furiya\n",
        "## Problem Statement 5 - DataBytes"
      ],
      "metadata": {
        "id": "8ANe80oZgE79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "3D5mvVdXISax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "QabNNMJvV3Pq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d misrakahmed/vegetable-image-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-R2Bup4LWxhM",
        "outputId": "2c0ce6f0-ff12-49bd-8ffc-bee7f6dbbad3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading vegetable-image-dataset.zip to /content\n",
            " 98% 526M/534M [00:06<00:00, 22.1MB/s]\n",
            "100% 534M/534M [00:06<00:00, 91.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install patool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORLZCGoqWzwA",
        "outputId": "59c36412-709e-4737-ba4d-4480aeebec47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting patool\n",
            "  Downloading patool-2.1.1-py2.py3-none-any.whl (94 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/94.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m92.2/94.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: patool\n",
            "Successfully installed patool-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import patoolib\n",
        "patoolib.extract_archive(\"/content/vegetable-image-dataset.zip\")\n",
        "!rm /content/vegetable-image-dataset.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tFhemolXDP9",
        "outputId": "b17cba8c-c492-4f1c-ac30-75ecd91a1ba9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO patool: Extracting /content/vegetable-image-dataset.zip ...\n",
            "INFO:patool:Extracting /content/vegetable-image-dataset.zip ...\n",
            "INFO patool: running /usr/bin/7z x -o./Unpack__6hi2hb8 -- /content/vegetable-image-dataset.zip\n",
            "INFO:patool:running /usr/bin/7z x -o./Unpack__6hi2hb8 -- /content/vegetable-image-dataset.zip\n",
            "INFO patool:     with input=''\n",
            "INFO:patool:    with input=''\n",
            "INFO patool: ... /content/vegetable-image-dataset.zip extracted to `Vegetable Images'.\n",
            "INFO:patool:... /content/vegetable-image-dataset.zip extracted to `Vegetable Images'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0wj7iJWWVKJJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os, shutil\n",
        "from PIL import Image\n",
        "from PIL import ImageEnhance\n",
        "from skimage.io import imread\n",
        "import random, pathlib, warnings, itertools, math, os, shutil\n",
        "from tensorflow.keras import layers\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "whBEYtSRIZiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= \"/content/Vegetable Images\"\n",
        "\n",
        "train_folder = os.path.join(dataset,\"train\")\n",
        "test_folder = os.path.join(dataset,\"validation\")\n",
        "validation_folder = os.path.join(dataset,\"test\")"
      ],
      "metadata": {
        "id": "0SwF4ZEYV04B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "trYQ1mwMQsW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_generator = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    rotation_range=10\n",
        ")\n",
        "\n",
        "val_data_generator = ImageDataGenerator(\n",
        "    rescale=1/255.,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "test_data_generator = ImageDataGenerator(rescale=1/255.)"
      ],
      "metadata": {
        "id": "YkQqFyObTc6E"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = train_data_generator.flow_from_directory(\n",
        "    train_folder,\n",
        "    shuffle=True,\n",
        "    class_mode='binary',\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "val_generator = val_data_generator.flow_from_directory(\n",
        "    validation_folder,\n",
        "    shuffle=False,\n",
        "    class_mode='binary',\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_generator = test_data_generator.flow_from_directory(\n",
        "    test_folder,\n",
        "    shuffle=False,\n",
        "    class_mode='binary',\n",
        "    batch_size=32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9ZvFbGSLsLr",
        "outputId": "68eab09a-2d7f-456e-8f93-d6e74ce195e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 15000 images belonging to 15 classes.\n",
            "Found 3000 images belonging to 15 classes.\n",
            "Found 3000 images belonging to 15 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_class_indices = train_generator.class_indices\n",
        "class_map = {v: k for k, v in train_class_indices.items()}\n",
        "class_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BfVY156Iy50",
        "outputId": "bdbffcb6-5fdd-4a77-a211-d7f5510b9183"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'Bean',\n",
              " 1: 'Bitter_Gourd',\n",
              " 2: 'Bottle_Gourd',\n",
              " 3: 'Brinjal',\n",
              " 4: 'Broccoli',\n",
              " 5: 'Cabbage',\n",
              " 6: 'Capsicum',\n",
              " 7: 'Carrot',\n",
              " 8: 'Cauliflower',\n",
              " 9: 'Cucumber',\n",
              " 10: 'Papaya',\n",
              " 11: 'Potato',\n",
              " 12: 'Pumpkin',\n",
              " 13: 'Radish',\n",
              " 14: 'Tomato'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.mobilenet_v2.MobileNetV2(\n",
        "    input_shape=[256,256,3], include_top=False,\n",
        "    classifier_activation='softmax',\n",
        ")\n",
        "pretrained.trainable = False\n",
        "model = tf.keras.models.Sequential([\n",
        "    pretrained,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(512, activation='relu'),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(15, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EULNXKNoQu-n",
        "outputId": "f2ad4499-4911-4899-8e47-63ce45e31277"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYHgUn-BbSqH",
        "outputId": "8c8d8d45-3f78-45ec-e77a-a840e9ece725"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_224 (Func  (None, 8, 8, 1280)        2257984   \n",
            " tional)                                                         \n",
            "                                                                 \n",
            " global_average_pooling2d (  (None, 1280)              0         \n",
            " GlobalAveragePooling2D)                                         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               655872    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                32832     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 15)                975       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2947663 (11.24 MB)\n",
            "Trainable params: 689679 (2.63 MB)\n",
            "Non-trainable params: 2257984 (8.61 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\"model.h5\", save_best_only=True)\n",
        "early_stopping_callback = tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\",patience=5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "bfpO-FijREod"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9tJPH6OnBorI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                    epochs=epochs,\n",
        "                    callbacks=[checkpoint_callback,early_stopping_callback])"
      ],
      "metadata": {
        "id": "h4qM3o5qRIw8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f01f4d7-3fa5-4657-9c71-b45e0a6dcbfe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "469/469 [==============================] - 320s 668ms/step - loss: 0.1177 - accuracy: 0.9675 - val_loss: 0.0274 - val_accuracy: 0.9910\n",
            "Epoch 2/15\n",
            "469/469 [==============================] - 274s 585ms/step - loss: 0.0304 - accuracy: 0.9912 - val_loss: 0.0423 - val_accuracy: 0.9877\n",
            "Epoch 3/15\n",
            "469/469 [==============================] - 272s 580ms/step - loss: 0.0204 - accuracy: 0.9941 - val_loss: 0.1042 - val_accuracy: 0.9730\n",
            "Epoch 4/15\n",
            "469/469 [==============================] - 273s 582ms/step - loss: 0.0217 - accuracy: 0.9942 - val_loss: 0.0328 - val_accuracy: 0.9920\n",
            "Epoch 5/15\n",
            "469/469 [==============================] - 311s 663ms/step - loss: 0.0154 - accuracy: 0.9954 - val_loss: 0.0138 - val_accuracy: 0.9947\n",
            "Epoch 6/15\n",
            "469/469 [==============================] - 278s 593ms/step - loss: 0.0110 - accuracy: 0.9965 - val_loss: 0.0093 - val_accuracy: 0.9970\n",
            "Epoch 7/15\n",
            "469/469 [==============================] - 275s 586ms/step - loss: 0.0149 - accuracy: 0.9955 - val_loss: 0.0231 - val_accuracy: 0.9917\n",
            "Epoch 8/15\n",
            "469/469 [==============================] - 273s 582ms/step - loss: 0.0175 - accuracy: 0.9951 - val_loss: 0.0231 - val_accuracy: 0.9913\n",
            "Epoch 9/15\n",
            "469/469 [==============================] - 270s 575ms/step - loss: 0.0136 - accuracy: 0.9957 - val_loss: 0.0068 - val_accuracy: 0.9973\n",
            "Epoch 10/15\n",
            "469/469 [==============================] - 272s 580ms/step - loss: 0.0096 - accuracy: 0.9974 - val_loss: 0.0156 - val_accuracy: 0.9960\n",
            "Epoch 11/15\n",
            "469/469 [==============================] - 272s 580ms/step - loss: 0.0065 - accuracy: 0.9981 - val_loss: 0.0190 - val_accuracy: 0.9947\n",
            "Epoch 12/15\n",
            "469/469 [==============================] - 272s 580ms/step - loss: 0.0082 - accuracy: 0.9981 - val_loss: 0.0081 - val_accuracy: 0.9970\n",
            "Epoch 13/15\n",
            "469/469 [==============================] - 271s 579ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0169 - val_accuracy: 0.9960\n",
            "Epoch 14/15\n",
            "469/469 [==============================] - 271s 579ms/step - loss: 0.0109 - accuracy: 0.9971 - val_loss: 0.0287 - val_accuracy: 0.9923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('/content/final_model.h5')\n",
        "# predicted_vegetable = generate_predictions(\"/content/batata.jpg\")\n",
        "# print(predicted_vegetable)"
      ],
      "metadata": {
        "id": "IJWwQiPkck2p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions(image_path):\n",
        "    test_img = image.load_img(image_path, target_size=(256, 256))\n",
        "    test_img_arr = image.img_to_array(test_img)/255.0\n",
        "    test_img_input = test_img_arr.reshape((1, test_img_arr.shape[0], test_img_arr.shape[1], test_img_arr.shape[2]))\n",
        "\n",
        "    predicted_label = np.argmax(model.predict(test_img_input))\n",
        "    predicted_vegetable = class_map[predicted_label]\n",
        "    return predicted_vegetable"
      ],
      "metadata": {
        "id": "GZFfapYxW-ap"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_predictions(\"/content/batata.jpg\")"
      ],
      "metadata": {
        "id": "SpdKckpcXJLq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "f11226bc-03b7-4d87-c2ca-7d5cb4ed0eb3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 22ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Potato'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantity (Object Detection)"
      ],
      "metadata": {
        "id": "Erl3sEOjLOeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow"
      ],
      "metadata": {
        "id": "fF-VBHOGLYGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "3gBXoItTVT_k"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "#replace (userdata.get('RoboFlowAPI') with oTFdgfpUXCaDBg8v2Is6 if not working\n",
        "# rf = Roboflow(api_key=(userdata.get('RoboFlowAPI')))\n",
        "rf = Roboflow(api_key=\"oTFdgfpUXCaDBg8v2Is6\")\n",
        "project = rf.workspace().project(\"test-veg\")\n",
        "ModelCount = project.version(1).model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RtXHqMNHLONq",
        "outputId": "b7ab38d7-49bb-4c16-d190-b8dda0b30bb9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_instances(predictions: dict) -> dict:\n",
        "    classes = {}\n",
        "\n",
        "    for bounding_box in predictions['predictions']:\n",
        "        if bounding_box['class'] in classes:\n",
        "            classes[bounding_box['class']] += 1\n",
        "        else:\n",
        "            classes[bounding_box['class']] = 1\n",
        "\n",
        "    return classes"
      ],
      "metadata": {
        "id": "-e2cOUIfSnwG"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = ModelCount.predict(\"/content/3_Potato.jpg\", confidence=40, overlap=30).save(\"prediction.jpg\")"
      ],
      "metadata": {
        "id": "f8gw8pXzS0R1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_instances(prediction)"
      ],
      "metadata": {
        "id": "jJvNe6TeNBHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating average weight\n"
      ],
      "metadata": {
        "id": "MotLYkgfSRUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_weight_for_predicted_vegetable(\n",
        "    predicted_vegetable, quantity_dict, csv_file_path\n",
        "):\n",
        "    df = pd.read_csv(csv_file_path, encoding=\"latin1\")\n",
        "\n",
        "    df[\"Product\"] = df[\"Product\"].str.lower()\n",
        "\n",
        "    product_to_weight = dict(zip(df[\"Product\"], df[\"Weight\"]))\n",
        "\n",
        "    predicted_vegetable_lower = predicted_vegetable.lower()\n",
        "\n",
        "    if predicted_vegetable_lower not in quantity_dict:\n",
        "        print(\"Error Predicting Vegetable: Predicted vegetable not found in quantity_dict.\")\n",
        "        return None\n",
        "\n",
        "    if predicted_vegetable_lower in product_to_weight:\n",
        "        predicted_quantity = quantity_dict.get(predicted_vegetable_lower, 0)\n",
        "        total_weight = predicted_quantity * product_to_weight[predicted_vegetable_lower]\n",
        "        return total_weight\n",
        "    else:\n",
        "        print(\n",
        "            f\"Warning: Predicted vegetable '{predicted_vegetable}' not found in the CSV file.\"\n",
        "        )\n",
        "        return None"
      ],
      "metadata": {
        "id": "MN58ol-cSVmN"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integration"
      ],
      "metadata": {
        "id": "jqrYnUlIWmUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_predictions_with_count(image_path):\n",
        "    predicted_vegetable = generate_predictions(image_path)\n",
        "    prediction = ModelCount.predict(image_path, confidence=40, overlap=30).json()\n",
        "    count = count_instances(prediction)\n",
        "    return predicted_vegetable, count\n"
      ],
      "metadata": {
        "id": "p5Ctd58yWlwr"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_path = \"/content/WeightMapping.csv\""
      ],
      "metadata": {
        "id": "Z4Aa89irXNe8"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "quantity_dict = {\n",
        "    'bean': 0, 'bitter_gourd': 0, 'bottle_gourd': 0, 'brinjal': 0,\n",
        "    'broccoli': 0, 'cabbage': 0, 'capsicum': 0, 'carrot': 0,\n",
        "    'cauliflower': 0, 'cucumber': 0, 'papaya': 0, 'potato': 0,\n",
        "    'pumpkin': 0, 'radish': 0, 'tomato': 0\n",
        "}\n",
        "\n",
        "list_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the list of vegetables and quantities (e.g., bean:3,carrot:2,tomato:4)',\n",
        "    description='Vegetable List:'\n",
        ")\n",
        "\n",
        "submit_button = widgets.Button(description='Submit')\n",
        "display(widgets.HBox([list_input, submit_button]))\n",
        "image_path_input = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='Enter the path to the image',\n",
        "    description='Image Path:'\n",
        ")\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_submit_click(b):\n",
        "    global items\n",
        "    items = None\n",
        "    try:\n",
        "        user_input = list_input.value\n",
        "        items = [item.split(':') for item in user_input.split(',')]\n",
        "        for vegetable, quantity in items:\n",
        "            quantity_dict[vegetable.strip().lower()] += int(quantity)\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            print(f\"Updated Purchase List: {quantity_dict}\")\n",
        "\n",
        "        display(image_path_input)\n",
        "\n",
        "    except ValueError:\n",
        "        with output:\n",
        "            output.clear_output()\n",
        "            print(\"Error: Invalid input format. Please enter quantities as integers.\")\n",
        "\n",
        "submit_button.on_click(on_submit_click)\n",
        "\n",
        "final_output = widgets.Output()\n",
        "display(final_output)\n",
        "\n",
        "def on_image_path_submit(b):\n",
        "    global items, quantity_dict\n",
        "    image_path = image_path_input.value\n",
        "    predicted_vegetable, count_dict = generate_predictions_with_count(image_path)\n",
        "    for vegetable, count in count_dict.items():\n",
        "        vegetable_lower = vegetable.lower()\n",
        "        if vegetable_lower in quantity_dict:\n",
        "            subtracted_count = min(count, quantity_dict[vegetable_lower])\n",
        "            quantity_dict[vegetable_lower] -= subtracted_count\n",
        "\n",
        "    predicted_vegetable_weight = calculate_weight_for_predicted_vegetable(\n",
        "        predicted_vegetable, count_dict, csv_path\n",
        "    )\n",
        "\n",
        "    with final_output:\n",
        "        final_output.clear_output()\n",
        "        if predicted_vegetable_weight is not None:\n",
        "            print(f\"Predicted Vegetable: {predicted_vegetable}\")\n",
        "            print(f\"Updated Purchase List: {quantity_dict}\")\n",
        "            print(f\"Total Weight: {predicted_vegetable_weight} grams\")\n",
        "        else:\n",
        "            print(\"Error calculating weight.\")\n",
        "\n",
        "\n",
        "image_path_submit_button = widgets.Button(description='Submit Image Path')\n",
        "image_path_submit_button.on_click(on_image_path_submit)\n",
        "display(image_path_submit_button)"
      ],
      "metadata": {
        "id": "zBuMYfCBlZNo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}